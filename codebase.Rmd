---
title: "Democracy & The Wealth of Nations"
author: "Jacob, Hugo and Jonathan"
date: "2023-02-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set working directory
knitr::opts_knit$set(root.dir = "/Users/jacoblillelund/Documents/applied-social-science/data")

library(readxl)
library(janitor)
library(tidyverse)
library(fastDummies)
library(fedmatch)#(https://cran.r-project.org/web/packages/fedmatch/vignettes/Fuzzy-matching.html)
library(broom)
```

# Load and clean data
```{r warning=FALSE}
### FREEDOM HOUSE ###
# import and clean
freedomhouse <- read_excel("All_data_FIW_2013-2022.xlsx", sheet = 2, skip = 1, col_names = TRUE)
freedomhouse <- freedomhouse %>% 
  clean_names() %>%                                            # tidy data with janitor-package
  rename(country = country_territory, year = edition ) %>%     # rename columns
  select(c("country", "region", "c_t", "year", "pr", "cl", "total"))  # select relevant columns

### DEMOCRACY MATRIX RANKING ###
democracy_matrix <- read_excel("democracy-matrix-ranking.xlsx", skip = 1, col_names = TRUE)
democracy_matrix <- democracy_matrix %>% 
  clean_names() %>%                  # tidy data with janitor-package
  select(country, total_value_index) # select columns

### WORLD DATA BANK MACROECONOMIC VARIABLES ###
# Import and clean
worlddatabank <- read_csv("worlddatabank.csv", show_col_types = FALSE) %>%  
  rename_at(vars(5:11), ~ substr(., 1, 4)) %>%   # rename "year"-columns
  mutate_at(vars(5:11), as.numeric) %>%          # convert observations to integers
  select(-c("Country Code")) %>%                 # exclude irrelevant column(s)
  rename("country" = "Country Name", economic_variable = "Series Name", series_code = "Series Code")
```

# Compare democracy-indexes
```{r warning=FALSE}
# Merge dataframes
# We compare 2020, as this is the year the democracy matrix is modelled on
index_comparison <- freedomhouse %>% filter(year == 2020) %>% 
  left_join(democracy_matrix, by = "country") %>% 
  select(country, total, total_value_index) %>%  # select relevant columns
  filter(!is.na(total_value_index)) %>% # filter for rows with missing values (territories are not included in Democracy Matrix ranking)
  mutate(total_value_index = total_value_index/10) # scale the democracy ranking to 0-100 from 0-1000

# Draw Scatterplot
index_comparison %>% 
  ggplot(aes(x = total, y = total_value_index, label = country)) +
  geom_text() +
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
# Check correlation
cor(index_comparison$total, index_comparison$total_value_index)
```

We can see that the freedomhouse democracy index has a very high correlation with the democracy matrix. 


# Create country-subsets of data ###

## EEA Subset (excluding Liechtenstein due to missing data)
```{r}
# Check that the country-names in the below string are exactly as in the world data bank
eea <- c("Austria", "Belgium", "Bulgaria", "Croatia", "Cyprus", "Czechia", 
                    "Denmark", "Estonia", "Finland", "France", "Germany", "Greece", "Hungary", 
                    "Iceland", "Ireland", "Italy", "Latvia", "Lithuania", 
                    "Luxembourg", "Malta", "Netherlands", "Norway", "Poland", "Portugal", 
                    "Romania", "Slovak Republic", "Slovenia", "Spain", "Sweden", "Switzerland")

# Check number of countries in string
length(eea)

# Filter relevant rows. The number of rows should equal length(eea)*number of variables
worlddatabank_eea <- worlddatabank %>% 
  filter(country %in% eea)
```

## EEA + OECD Subset
```{r}
# List OECD countries 
oecd <- c("Australia", "Austria", "Belgium", "Canada", "Chile", "Colombia",
              "Czechia", "Denmark", "Estonia", "Finland", "France", "Germany",
              "Greece", "Hungary", "Iceland", "Ireland", "Israel", "Italy", "Japan",
              "Korea, Rep.", "Latvia", "Lithuania", "Luxembourg", "Mexico", "Netherlands",
              "New Zealand", "Norway", "Poland", "Portugal", "Slovak Republic", "Slovenia",
              "Spain", "Sweden", "Switzerland", "Turkiye", "United Kingdom", "United States",
              "Costa Rica")

#Create unique string of oecd+eea countries
oecd_eea <- unique(c(oecd, eea))

# Check number of countries in string
length(oecd_eea)
  
# Filter relevant rows. The number of rows should equal length(oecd_eea)*number of variables
worlddatabank_oecd_eea  <- worlddatabank %>% 
  filter(country %in% oecd_eea)
```

# Missing data calculations for each subset

```{r}
# Extract variable names
economic_variable <- unique(na.omit(worlddatabank[,2],)) # extract variable strings
```

## All countries
```{r}
# Initiate missing-data dataframe 
missing_data_allcountries <- tibble(economic_variable)


for (unique_variable in missing_data_allcountries$economic_variable) {
  
  for (year in c("2013", "2014", "2015", "2016", "2017", "2018", "2019")) {
  
  na <- worlddatabank %>%
            filter(economic_variable == unique_variable) %>% 
            select(year) %>% 
            summarize(num_na = sum(is.na(.)))
  total <- worlddatabank %>%
            filter(economic_variable == unique_variable) %>% 
            select(year) %>% 
            nrow()

  missing_percentage <- as.numeric(na/total)

  missing_data_allcountries[[year]][missing_data_allcountries$economic_variable == unique_variable] <- missing_percentage
  }
  
}

missing_data_allcountries <- rbind(missing_data_allcountries, c("Mean Missing Percentage", colMeans(missing_data_allcountries[1:5,2:8])))

missing_data_allcountries
```

## EEA
```{r}
# Initiate missing-data dataframe 
missing_data_eea <- tibble(economic_variable)


for (unique_variable in missing_data_eea$economic_variable) {
  
  for (year in c("2013", "2014", "2015", "2016", "2017", "2018", "2019")) {
  
  na <- worlddatabank_eea %>%
            filter(economic_variable == unique_variable) %>% 
            select(year) %>% 
            summarize(num_na = sum(is.na(.)))
  total <- worlddatabank_eea %>%
            filter(economic_variable == unique_variable) %>% 
            select(year) %>% 
            nrow()

  missing_percentage <- as.numeric(na/total)

  missing_data_eea[[year]][missing_data_eea$economic_variable == unique_variable] <- missing_percentage
  }
  
}

missing_data_eea <- rbind(missing_data_eea, c("Mean Missing Percentage", colMeans(missing_data_eea[1:5,2:8])))

missing_data_eea
```

## EEA + OECD
```{r}
# Initiate missing-data dataframe 
missing_data_oecd_eea <- tibble(economic_variable)


for (unique_variable in missing_data_oecd_eea$economic_variable) {
  
  for (year in c("2013", "2014", "2015", "2016", "2017", "2018", "2019")) {
  
  na <- worlddatabank_oecd_eea %>%
            filter(economic_variable == unique_variable) %>% 
            select(year) %>% 
            summarize(num_na = sum(is.na(.)))
  total <- worlddatabank_oecd_eea %>%
            filter(economic_variable == unique_variable) %>% 
            select(year) %>% 
            nrow()

  missing_percentage <- as.numeric(na/total)

  missing_data_oecd_eea[[year]][missing_data_oecd_eea$economic_variable == unique_variable] <- missing_percentage
  }
  
}

missing_data_oecd_eea <- rbind(missing_data_oecd_eea, c("Mean Missing Percentage", colMeans(missing_data_oecd_eea[1:5,2:8])))

missing_data_oecd_eea
```

# Modelling
Based on the missing data for the macroeconomic variables, we have decided to model based on OECD + EEA countries for 2019 and excluding the Gini-coefficient-variable. 

For our  multiple regression model we aim to model based on data for one given year, and not time-series data. For the years 2020, 2021, and 2022 the data is very incomplete. For 2020 and 2021, this intuitively seems to be a result of the COVID-19 pandemic, and for 2022 simply just, that the data was not released by the time we gathered data (beginning of 2023). We suggest that the rather incomplete data for the years under COVID-19 are not representative of the macro-economy in a normal year. 

Despite having more complete data in years preceeding 2019, we do still believe it makes more sense to model on data from the most recent representative year. GIven the above reasons, we suggest and justify 2019 to be the most relevant year to model on.

Furthermore, as we have settled on a broader set of countries than solely EEA and through our research have discovered that the Gini-coefficient are not necessarily reported using the same standards of calculations, we exclude this variable in our model.

## Filter final dataset to only include 2019 data excluding the Gini-variable
```{r}
worlddatabank_oecd_eea_2019 <- worlddatabank_oecd_eea %>%
  select(country, economic_variable, `series_code`, '2019') %>% 
  filter(economic_variable != "Gini index") %>% 
  rename(value = year )

# Convert into wide-format for later modelling and exploration of data 
worlddatabank_oecd_eea_2019<- worlddatabank_oecd_eea_2019 %>%
  pivot_wider(names_from = economic_variable, values_from = value, id_cols = "country") %>% 
  clean_names() # clean column names
```

## Filter freedomhouse-data for 2019 only
```{r}
freedomhouse_2019 <- freedomhouse %>%
  filter(year == 2019)
```

## Check for multicollinearity between variables
Checking for multicollinearty between the variables is vital as it would impair our research if there was collinearity between variables, as collinear variables can predict each other. 

We test multicollinearity using the Variance Inflation Factor (VIF), VIF tests the correlation between the independent variables and the strength of that correlation. It is predicted by regressing a variable against every other variable, and explains how well the variable is explained by other variables. Variance inflation factor is calculated as 1 divided by the tolerance. The tolerance is the percent of variance in the variable that cannot be accounted for by other variables and is calculated as 1 minus R squared. The VIF score starts at 1 and has no upper limit, a value of 1 indicates that there is no correlation between variables and therefore no collinearity. A score between 1 and 5 suggests a moderate collinearity, but not enough to take corrective action. Above 5 is serious and warrants corrective measures, as the severity of the potential problems increase.
```{r}
# remove non-numeric variables, that are irrelevant for collinearity analysis
worlddatabank_oecd_eea_numeric <- worlddatabank_oecd_eea_2019 %>% select(-c("country"))

names(worlddatabank_oecd_eea_numeric)

```

```{r}
# calculate correlation matrix
corr_matrix <- cor(worlddatabank_oecd_eea_numeric)

# calculate VIF values for each variable
vif_values <- vif(lm(gdp_growth_annual_percent ~ gdp_per_capita_constant_2015_us + research_and_development_expenditure_percent_of_gdp + tax_revenue_percent_of_gdp + total_natural_resources_rents_percent_of_gdp, data = worlddatabank_oecd_eea_numeric))

# add VIF values to correlation matrix data frame
corr_matrix_with_vif <- cbind(corr_matrix, VIF = vif_values)

# format correlation matrix with VIF values as table
corr_table <- corr_matrix_with_vif %>%
  as.data.frame() %>%
  round(2) %>%
  kable(format = "html", digits = 2, caption = "VIF Matrix") %>%
  kable_paper("hover", full_width = F)
  
# print table
print(corr_table)
```

```{r}
#calculate VIF  scores with each predictor as the dependent variable
vif_values_gdp_growth <- vif(lm(gdp_growth_annual_percent ~ gdp_per_capita_constant_2015_us + research_and_development_expenditure_percent_of_gdp + tax_revenue_percent_of_gdp + total_natural_resources_rents_percent_of_gdp, data = worlddatabank_oecd_eea_numeric))

vif_values_gdp_capita <- vif(lm(gdp_per_capita_constant_2015_us ~ research_and_development_expenditure_percent_of_gdp + tax_revenue_percent_of_gdp + total_natural_resources_rents_percent_of_gdp + gdp_growth_annual_percent, data = worlddatabank_oecd_eea_numeric))

vif_values_rnd <- vif(lm(research_and_development_expenditure_percent_of_gdp ~ tax_revenue_percent_of_gdp + total_natural_resources_rents_percent_of_gdp + gdp_growth_annual_percent + gdp_per_capita_constant_2015_us, data = worlddatabank_oecd_eea_numeric))

vif_values_tax <- vif(lm(tax_revenue_percent_of_gdp ~ total_natural_resources_rents_percent_of_gdp + gdp_growth_annual_percent + gdp_per_capita_constant_2015_us + research_and_development_expenditure_percent_of_gdp, data = worlddatabank_oecd_eea_numeric))

vif_values_tnr <- vif(lm(total_natural_resources_rents_percent_of_gdp ~ gdp_growth_annual_percent + gdp_per_capita_constant_2015_us + research_and_development_expenditure_percent_of_gdp + tax_revenue_percent_of_gdp, data = worlddatabank_oecd_eea_numeric))
```

```{r}
# Make a matrix of all VIF scores
vif_matrix <- cbind(vif_values_gdp_capita, vif_values_gdp_growth, vif_values_rnd, vif_values_tax, vif_values_tnr)

corr_table_vif <- vif_matrix %>%
  as.data.frame() %>%
  round(2) %>%
  kable(format = "html", digits = 2, caption = "VIF Matrix") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

print(corr_table_vif)
```
As all of our variables have a VIF score of just above 1, it means that no further investigation is warranted and all correlation coeffiecients and p-values can be trusted.

## Merge our final World Data Bank dataset with the freedomhouse dataset
Merging the dataset worlddatabank_oecd_eea_2019 and freedomhouse_2019 creates issues, as the same naming conventions have not been followed on different countries. For instance, "Turkiye" from the world data bank is equivalent to the "Turkey"-entry in the freedomhouse-data

This is solved used the fedmatch package, that allows "fuzzy" merging. An alternative option is using dirty_cat in Python. Example script provided in the repository (One would need to export the dataframes as csv files to load into python)

INSERT EXPLANATION OF FUNCTION

```{r}
# Add unique key for each observation to both datasets for fedmatch::merge_plus (function requirement)

worlddatabank_oecd_eea_2019$unique_key_1 <- 1:nrow(worlddatabank_oecd_eea_2019)
freedomhouse_2019$unique_key_2 <- 1:nrow(freedomhouse_2019)

# Merge datasets using merge_plus and the Weighted Jaccard algorithm -
  # NB: Order of dataset matters
fuzzy_result <- merge_plus(data1 = worlddatabank_oecd_eea_2019, 
                          data2 = freedomhouse_2019,
                          by.x = "country",
                          by.y = "country", match_type = "fuzzy", 
                          fuzzy_settings = build_fuzzy_settings(method = "jw", maxDist = .5),
                          unique_key_1 = "unique_key_1",
                          unique_key_2 = "unique_key_2") 

# Check if any rows did not get a match
print(fuzzy_result$data1_nomatch)

```
All 43 rows was matched. To make sure that the matching has been done correctly, we inspect those rows that did not have an exact match and verify the match:

```{r}
# Inspect non-exact matches
fuzzy_result$matches %>%
  filter(country_1 != country_2) %>%
  select(country_1, country_2)
```

All matches but one was done correctly - Korea, Rep. was not matched correctly. A manual inspection of the freedomhouse-data set shows that this is due to different naming: "South Korea".  
This is manually corrected:
```{r}
freedomhouse_2019 <- freedomhouse_2019 %>%
  mutate(country = if_else(country == "South Korea", "Korea, Rep.", country))
```

We can then rerun the script and inspect again:

```{r}
fuzzy_result <- merge_plus(data1 = worlddatabank_oecd_eea_2019, 
                          data2 = freedomhouse_2019,
                          by.x = "country",
                          by.y = "country", match_type = "fuzzy", 
                          fuzzy_settings = build_fuzzy_settings(method = "jw", maxDist = .5),
                          unique_key_1 = "unique_key_1",
                          unique_key_2 = "unique_key_2") 

# Inspect non-exact matches
fuzzy_result$matches %>%
  filter(country_1 != country_2) %>%
  select(country_1, country_2)
```
The non-exact matches are all correct, so we save the results to a dataframe and select relevant rows

```{r}
# Save matches to data frame
modelling_data <- as.data.frame(fuzzy_result$matches)

# Select and rearrange relevant rows:
modelling_data <- modelling_data %>% 
  select("country_1", 
        "gdp_growth_annual_percent", 
        "gdp_per_capita_constant_2015_us",
        "research_and_development_expenditure_percent_of_gdp",
        "tax_revenue_percent_of_gdp",
        "total_natural_resources_rents_percent_of_gdp",
        "total") %>% 
  rename(democracy_score = total, country = country_1)
```

# Exploring the data

... Consider facet wrapping over the variable and make 6 plots in 1

## Histogram of Democracy Score distribution

```{r}
ggplot(modelling_data, aes(x = democracy_score)) + # map the democracy score variable
  geom_histogram(fill = "steelblue") +  # create histogram with fill color
  geom_vline(aes(xintercept = mean(democracy_score)), linetype = 2) + #creating dashed line at x=0
  theme_minimal() + 
  ylab("Percent-points") + 
  xlab("Democracy Score")
```

## GDP growth

### Histogram
```{r}
ggplot(modelling_data, aes(x = gdp_growth_annual_percent)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(gdp_growth_annual_percent)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot

```{r}
ggplot(modelling_data, aes(x = gdp_growth_annual_percent, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "GDP growth (annual %)", y = "Total") +
  ggtitle("Democracy Score vs. GDP growth") +
  theme_minimal()
```

## GDP pr capita

### Histogram
```{r}
ggplot(modelling_data, aes(x = gdp_per_capita_constant_2015_us)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(gdp_per_capita_constant_2015_us)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot

```{r}
ggplot(modelling_data, aes(x = gdp_per_capita_constant_2015_us, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "GDP per capita (constant 2015 US$)", y = "Total") +
  ggtitle("Democracy Score vs. GDP per capita") +
  theme_minimal()
```

## R&D expenditure

### Histogram
```{r}
ggplot(modelling_data, aes(x = research_and_development_expenditure_percent_of_gdp)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(research_and_development_expenditure_percent_of_gdp)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```

### Scatterplot

```{r}
ggplot(modelling_data, aes(x = research_and_development_expenditure_percent_of_gdp, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Research and development expenditure (% of GDP)", y = "Total") +
  ggtitle("Democracy Score vs. R&D expenditure") +
  theme_minimal()
```

## Tax Revenue

### Histogram
```{r}
ggplot(modelling_data, aes(x = tax_revenue_percent_of_gdp)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(tax_revenue_percent_of_gdp)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot

```{r}
ggplot(modelling_data, aes(x = tax_revenue_percent_of_gdp, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Tax revenue (% of GDP)", y = "Total") +
  ggtitle("Democracy Score vs. Tax revenue") +
  theme_minimal()
```

## Natural resources

### Histogram 
```{r}
ggplot(modelling_data, aes(x = total_natural_resources_rents_percent_of_gdp)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(total_natural_resources_rents_percent_of_gdp)), linetype = 2) +
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot
```{r}
ggplot(modelling_data, aes(x = total_natural_resources_rents_percent_of_gdp, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Total natural resources rents (% of GDP)", y = "Total") +
  ggtitle("Democracy Score vs. Natural resources rents") +
  theme_minimal()
```

# Creating model(s)

As the scales of measurement differs greatly between the variables, we normalize them all to be able to compare their coefficients:

```{r}
# Select the predictor variables to be normalized
predictor_vars <- c("gdp_growth_annual_percent","gdp_per_capita_constant_2015_us",
        "research_and_development_expenditure_percent_of_gdp",
        "tax_revenue_percent_of_gdp",
        "total_natural_resources_rents_percent_of_gdp")

# Normalize the predictor variables and save to a new dataframe
modelling_data_norm <- modelling_data
modelling_data_norm[predictor_vars] <- scale(modelling_data_norm[predictor_vars])
```

Each value in the variable-columns now represent how many standard deviations the given country falls from the mean in regards to that variable.

We can then fit the linear regression model
```{r}
# Fit the linear regression model with the normalized predictor variables
democracy_score_model_norm <- lm(democracy_score ~ 
                                  gdp_growth_annual_percent + 
                                  gdp_per_capita_constant_2015_us  +
                                  research_and_development_expenditure_percent_of_gdp +
                                  tax_revenue_percent_of_gdp +
                                  total_natural_resources_rents_percent_of_gdp,
                                data = modelling_data_norm)

tidy(democracy_score_model_norm) 
glance(democracy_score_model_norm)
```
The model is not very good..? 

We attempt to model only by GDP per Capita and Tax Revenue

```{r}
democracy_score_model_gdp_tax <- lm(democracy_score ~ gdp_per_capita_constant_2015_us + tax_revenue_percent_of_gdp,
                                 data = modelling_data_norm)

tidy(democracy_score_model_gdp_tax) 
glance(democracy_score_model_gdp_tax)

```

We attempt to model only by GDP per capita:
```{r}

# Fit the linear regression model with normalized GDP constant
democracy_score_model_gdp <- lm(democracy_score ~ gdp_per_capita_constant_2015_us,
                                 data = modelling_data_norm)

tidy(democracy_score_model_gdp) 
glance(democracy_score_model_gdp)
```

Modelled only on tax revenue
```{r}
# Fit the linear regression model with normalized Tax Revenue constant
democracy_score_model_tax_revenue <- lm(democracy_score ~ tax_revenue_percent_of_gdp,
                                 data = modelling_data_norm)

tidy(democracy_score_model_tax_revenue) 
glance(democracy_score_model_tax_revenue)
```

# Check assumptions
Check assumptions such as linearity, normality, and homoscedasticity to ensure that the model is appropriate for your data.
