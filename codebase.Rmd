---
title: "Democracy & The Wealth of Nations"
author: "Jacob, Hugo and Jonathan"
date: "2023-02-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set working directory
knitr::opts_knit$set(root.dir = "/Users/jacoblillelund/Documents/applied-social-science/data")

library(readxl)
library(janitor)
library(tidyverse)
library(fastDummies)
library(fedmatch)#(https://cran.r-project.org/web/packages/fedmatch/vignettes/Fuzzy-matching.html)
library(broom)
```

# Load and clean data
```{r warning=FALSE}
### FREEDOM HOUSE ###
# import and clean
freedomhouse <- read_excel("All_data_FIW_2013-2022.xlsx", sheet = 2, skip = 1, col_names = TRUE)
freedomhouse <- freedomhouse %>% 
  clean_names() %>%                                            # tidy data with janitor-package
  rename(country = country_territory, year = edition ) %>%     # rename columns
  select(c("country", "region", "c_t", "year", "pr", "cl", "total"))  # select relevant columns

### DEMOCRACY MATRIX RANKING ###
democracy_matrix <- read_excel("democracy-matrix-ranking.xlsx", skip = 1, col_names = TRUE)
democracy_matrix <- democracy_matrix %>% 
  clean_names() %>%                  # tidy data with janitor-package
  select(country, total_value_index) # select columns

### WORLD DATA BANK MACROECONOMIC VARIABLES ###
# Import and clean
worlddatabank <- read_csv("worlddatabank.csv", show_col_types = FALSE) %>%  
  rename_at(vars(5:13), ~ substr(., 1, 4)) %>%   # rename "year"-columns
  rename("country" = "Country Name", variable = "Series Name") %>% 
  mutate_at(vars(5:13), as.numeric) %>%          # convert observations to integers
  select(-c("Country Code", "Series Code"))      # exclude irrelevant column(s)
```

# Compare democracy-indexes
```{r warning=FALSE}
# Merge dataframes
# We compare 2020, as this is the year the democracy matrix is modelled on
index_comparison <- freedomhouse %>% filter(year == 2020) %>% 
  left_join(democracy_matrix, by = "country") %>% 
  select(country, total, total_value_index) %>%  # select relevant columns
  filter(!is.na(total_value_index)) %>% # filter for rows with missing values (territories are not included in Democracy Matrix ranking)
  mutate(total_value_index = total_value_index/10) # scale the democracy ranking to 0-100 from 0-1000

# Draw Scatterplot
index_comparison %>% 
  ggplot(aes(x = total, y = total_value_index, label = country)) +
  geom_text() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
# Check correlation
cor(index_comparison$total, index_comparison$total_value_index)
```

We can see that the freedomhouse democracy index has a very high correlation with the democracy matrix. 


# Missing data calculations for each subset
INSERT FUNCTION

```{r}
# Extract variable names
variable <- unique(na.omit(worlddatabank[,2],)) # extract variable strings
```

```{r}
# The datafile has to be in a specific format in order to work with the function. The format goes like this: The first column is supposed to be the country, the second column is the different variable for each country, and the following columns are supposed to be measurements per year. The code beneath this is an example on how this can be achieved.

worlddatabank <- read_csv("worlddatabank.csv", show_col_types = FALSE)

# First we extract the column with the contry name, and the series name, which in this case is our variable
worlddatabank2 <- worlddatabank[, c("Country Name", "Series Name")]

# We then circumnavigate some unused columns by indexinf from the 5ft column, which is where our measuremnts per year starts, and bind this to our previusly created dataframe
worlddatabank2 <- cbind(worlddatabank2, worlddatabank[, 5:ncol(worlddatabank)])

# We now have to make sure, that the columns have the correct names. This is the "manual" part of this process
worlddatabank2 <- setNames(worlddatabank2, c("Country", "variable", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021"))

# Now the data file should be ready for an N/A assessment

```

```{r}

missing_data_function <- function(data_file) {

  data_file[, 3:ncol(data_file)] <- lapply(data_file[, 3:ncol(data_file)], as.numeric)
  
  col_names <- colnames(data_file)

  years <- col_names[3:length(col_names)]

  variables <- unique(na.omit(data_file[,2]))

  missing_data <- data.frame(variable = variables)

  for (v in missing_data$variable) {
    for (year in years) {
  
      n_na <- data_file %>%
        filter(variable == v) %>% 
        select(year) %>% 
        summarize(num_na = sum(is.na(.)))
      
      n_total <- data_file %>%
        filter(variable == v) %>% 
        select(year) %>% 
        nrow()

      md <- as.numeric(n_na/n_total)

      missing_data[missing_data$variable == v, year] <- md
    }
  }

  row_means <- rowMeans(missing_data[1:nrow(missing_data),2:ncol(missing_data)], na.rm = TRUE)
  col_means <- colMeans(missing_data[1:nrow(missing_data),2:ncol(missing_data)], na.rm = TRUE)

  missing_data <- rbind(missing_data, c("mean col", col_means))
  missing_data$mean_row <- c(row_means, NA)
  
  df_col_means <- data.frame(years, col_means)

  col_means_plot <- ggplot(df_col_means, aes(x = years, y = col_means, fill = years)) +
                    geom_bar(stat = "identity") +
                    labs(x = "Year", y = "Missing Data", title = "Missing Data per year")
  

  df_row_means <- data.frame(variables, row_means)

  row_means_plot <- ggplot(df_row_means, aes(x = variables, y = row_means, fill = variables)) +
                    geom_bar(stat = "identity") +
                    labs(x = "Variable", y = "Missing Data", title = "Missing data per variable") +
                    theme(axis.text.x = element_text(angle = 90, size = 8, hjust = 1))



  

list(missing_data, col_means_plot, row_means_plot)
}

missing_data_function(worlddatabank)
```

```

# Modelling
Based on the missing data for the macroeconomic variables, we have decided to model based on year 2019 and for the four variables for which we have more than 50% of the data available.

For our  multiple regression model we aim to model based on data for one given year, and not time-series data. For the years 2020, 2021, and 2022 the data is very incomplete. For 2020 and 2021, this intuitively seems to be a result of the COVID-19 pandemic, and for 2022 simply just, that the data was not released by the time we gathered data (beginning of 2023). We suggest that the rather incomplete data for the years under COVID-19 are not representative of the macro-economy in a normal year. 

Despite having more complete data in years preceeding 2019, we do still believe it makes more sense to model on data from the most recent representative year. Given the above reasons, we suggest and justify 2019 to be the most relevant year to model on.

As we are missing 58% of data for the R&D variable and 74% for the Gini-variable, we have decided to exclude these in our model. Furthermore, our research shows us that there a different standards for reporting Gini between countries.

## Check for multicollinearity between variables
Checking for multicollinearty between the variables is vital as it would impair our research if there was collinearity between variables, as collinear variables can predict each other. 

We test multicollinearity using the Variance Inflation Factor (VIF), VIF tests the correlation between the independent variables and the strength of that correlation. It is predicted by regressing a variable against every other variable, and explains how well the variable is explained by other variables. Variance inflation factor is calculated as 1 divided by the tolerance. The tolerance is the percent of variance in the variable that cannot be accounted for by other variables and is calculated as 1 minus R squared. The VIF score starts at 1 and has no upper limit, a value of 1 indicates that there is no correlation between variables and therefore no collinearity. A score between 1 and 5 suggests a moderate collinearity, but not enough to take corrective action. Above 5 is serious and warrants corrective measures, as the severity of the potential problems increase.
```{r}
# remove non-numeric variables, that are irrelevant for collinearity analysis
worlddatabank_2019_numeric <- worlddatabank_2019 %>% select(-c("country"))

names(worlddatabank_2019_numeric)

```

```{r}
calculate_vif <- function(data) {
  
  predictors <- names(data)[sapply(data, is.numeric)]
  
  vif_results <- data.frame(variable = character(),
                             vif = double(),
                             stringsAsFactors = FALSE)
  
  for (p in predictors) {
    vif_value <- vif(lm(paste(p, "~ .", sep = ""), data = data))
    vif_results <- rbind(vif_results, data.frame(variable = p,
                                                 vif = vif_value,
                                                 stringsAsFactors = FALSE))
  }
  
  return(vif_results)
}

my_vif_results <- calculate_vif(worlddatabank_2019_numeric)

my_vif_results <- unstack(my_vif_results, form=vif~variable)

for (i in seq_along(names(worlddatabank_2019_numeric)[-1])) {
  rownames(my_vif_results)[i] <- names(worlddatabank_2019_numeric)[-1][i]
}

```

```{r}
# Make a matrix of all VIF scores
vif_matrix <- cbind(my_vif_results)

corr_table_vif <- vif_matrix %>%
  as.data.frame() %>%
  round(2) %>%
  kable(format = "html", digits = 2, caption = "VIF Matrix") %>%
  kable_styling(bootstrap_options = c("hover", full_width = F))

print(corr_table_vif)
```
As all of our variables have a VIF score of just above 1, it means that no further investigation is warranted and all correlation coeffiecients and p-values can be trusted.

## Filter freedomhouse-data for 2019 only
```{r}
freedomhouse_2019 <- freedomhouse %>%
  filter(year == 2019)
```

## Merge our final World Data Bank dataset with the freedomhouse dataset
Merging the dataset worlddatabank_2019 and freedomhouse_2019 creates issues, as the same naming conventions have not been followed on different countries. For instance, "Turkiye" from the world data bank is equivalent to the "Turkey"-entry in the freedomhouse-datas

This is solved used the fedmatch package, that allows "fuzzy" merging. An alternative option is using dirty_cat in Python. 

## Filter final dataset to only include 2019 data for all countries excluding the Gini-variable and R&D
```{r}
worlddatabank_2019 <- worlddatabank %>%
  select(country, variable, '2019') %>% 
  filter(variable != "Gini index", variable != "Research and development expenditure (% of GDP)") %>% 
  rename(value = year )

# Convert into wide-format for later modelling and exploration of data 
worlddatabank_2019 <-  worlddatabank_2019 %>%
  pivot_wider(names_from = variable, values_from = value, id_cols = "country") %>% 
  clean_names() %>%  # clean column names 
  slice(1:(which(country == "Zimbabwe")))     # Include rows up to and including Zimbabwe (the rows following are groupings of countries)
```

## Merge our final World Data Bank dataset with the freedomhouse dataset
We still have the dataset freedomhouse_2019 which we can merge with our data for all countries

Merging as in the previous example using fedmatch::merge_plus

INSERT EXPLANATION OF FUNCTION

```{r}
# Add unique key for each observation to both datasets for fedmatch::merge_plus (function requirement)
worlddatabank_2019$unique_key_1 <- 1:nrow(worlddatabank_2019)
freedomhouse_2019$unique_key_2 <- 1:nrow(freedomhouse_2019)
# we already have a unique_key_2 column in the freedomhouse dataset

# Merge datasets using merge_plus and the Weighted Jaccard algorithm -
  # NB: Order of dataset matters
fuzzy_result <- merge_plus(data1 = worlddatabank_2019, 
                          data2 = freedomhouse_2019,
                          by.x = "country",
                          by.y = "country", match_type = "fuzzy", 
                          fuzzy_settings = build_fuzzy_settings(method = "jw", maxDist = .2),
                          unique_key_1 = "unique_key_1",
                          unique_key_2 = "unique_key_2") 

# Check if any rows did not get a match
print(fuzzy_result$data1_nomatch)

```

22 rows did not have a match. These have been manually inspected, and only 6 of these are suppossed to be matched with a corresponding row in the freedomhouse data-set, namely:

"South Korea", "Korea, Rep."
"Samoa", "American Samoa"
"Congo (Kinshasa)", "Congo, Dem. Rep."
"Congo (Brazzaville)", "Congo, Rep."
"The Gambia", "Gambia, The"
"North Korea", "Korea, Dem. People's Rep."

That is manually renamed:

```{r}
freedomhouse_2019 <- freedomhouse_2019 %>%
  mutate(country = case_when(
    country == "South Korea" ~ "Korea, Rep.",
    country == "Samoa" ~ "American Samoa",
    country == "Congo (Kinshasa)" ~ "Congo, Dem. Rep.",
    country == "Congo (Brazzaville)" ~ "Congo, Rep.",
    country == "The Gambia" ~ "Gambia, The",
    country == "North Korea" ~ "Korea, Dem. People's Rep.",
    TRUE ~ country))
```

We can then rerun the script and inspect again:

```{r}

fuzzy_result<- merge_plus(data1 = worlddatabank_2019, 
                          data2 = freedomhouse_2019,
                          by.x = "country",
                          by.y = "country", match_type = "fuzzy", 
                          fuzzy_settings = build_fuzzy_settings(method = "jw", maxDist = .2),
                          unique_key_1 = "unique_key_1",
                          unique_key_2 = "unique_key_2") 

# Check the rows that did not get an exact match
print(fuzzy_result$data1_nomatch)
```
We do not expect these 16 rows to be matched, as the freedomhouse dataset does not contain democracy scores for these entities.

```{r}
# Inspect non-exact matches
fuzzy_result$matches %>%
  filter(country_1 != country_2) %>%
  select(country_1, country_2)
```

Of the non-exact matches, a couple of them are wrongly matched:

1. Greenland is matched with	Grenada
2. Guam	is matched with Guatemala
3. New Caledonia is matched with	New Zealand
4. Northern Mariana Islands is matched with	Northern Cyprus

There are no correct row from the democracy index to be matched with, so we exclude these rows.

```{r}
# Save matches to data frame and exclude the relevant rows
modelling_data <- as.data.frame(fuzzy_result$matches) %>% 
  filter(!(country_1 %in% c("Greenland", "Guam", "New Caledonia", "Northern Mariana Islands")))
```


```{r}
# Select and rearrange relevant rows:
modelling_data <- modelling_data %>% 
  select("country_1", 
        "gdp_growth_annual_percent", 
        "gdp_per_capita_constant_2015_us",
        "tax_revenue_percent_of_gdp",
        "total_natural_resources_rents_percent_of_gdp",
        "total") %>% 
  rename(democracy_score = total, country = country_1)
```

# Exploring the data

## Histogram of Democracy Score distribution

```{r}
ggplot(modelling_data, aes(x = democracy_score)) + # map the democracy score variable
  geom_histogram(fill = "steelblue") +  # create histogram with fill color
  geom_vline(aes(xintercept = mean(democracy_score)), linetype = 2) + #creating dashed line at x=0
  theme_minimal() + 
  ylab("Percent-points") + 
  xlab("Democracy Score")
```

## GDP growth

### Histogram
```{r}
ggplot(modelling_data, aes(x = gdp_growth_annual_percent)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(gdp_growth_annual_percent)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot

```{r}
ggplot(modelling_data, aes(x = gdp_growth_annual_percent, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "GDP growth (annual %)", y = "Total") +
  ggtitle("Democracy Score vs. GDP growth") +
  theme_minimal()
```

## GDP pr capita

### Histogram
```{r}
ggplot(modelling_data, aes(x = gdp_per_capita_constant_2015_us)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(gdp_per_capita_constant_2015_us)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot

```{r}
ggplot(modelling_data, aes(x = gdp_per_capita_constant_2015_us, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "GDP per capita (constant 2015 US$)", y = "Total") +
  ggtitle("Democracy Score vs. GDP per capita") +
  theme_minimal()
```

## Tax Revenue

### Histogram
```{r}
ggplot(modelling_data, aes(x = tax_revenue_percent_of_gdp)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(tax_revenue_percent_of_gdp)), linetype = 2) + 
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot

```{r}
ggplot(modelling_data, aes(x = tax_revenue_percent_of_gdp, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Tax revenue (% of GDP)", y = "Total") +
  ggtitle("Democracy Score vs. Tax revenue") +
  theme_minimal()
```

## Natural resources

### Histogram 
```{r}
ggplot(modelling_data, aes(x = total_natural_resources_rents_percent_of_gdp)) + 
  geom_histogram(fill = "steelblue") +  
  geom_vline(aes(xintercept = mean(total_natural_resources_rents_percent_of_gdp)), linetype = 2) +
  theme_minimal() + 
  ylab("Number of countries")
```
### Scatterplot
```{r}
ggplot(modelling_data, aes(x = total_natural_resources_rents_percent_of_gdp, y = democracy_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Total natural resources rents (% of GDP)", y = "Total") +
  ggtitle("Democracy Score vs. Natural resources rents") +
  theme_minimal()
```

# Creating model(s)

As the scales of measurement differs greatly between the variables, we normalize them all to be able to compare their coefficients:

```{r}
# Select the predictor variables to be normalized
predictor_vars <- c("gdp_growth_annual_percent","gdp_per_capita_constant_2015_us",
        "tax_revenue_percent_of_gdp",
        "total_natural_resources_rents_percent_of_gdp")

# Normalize the predictor variables and save to a new dataframe
modelling_data_norm <- modelling_data
modelling_data_norm[predictor_vars] <- scale(modelling_data_norm[predictor_vars])
```

Each value in the variable-columns now represent how many standard deviations the given country falls from the mean in regards to that variable.

We can then fit the linear regression model
```{r}
# Fit the linear regression model with the normalized predictor variables
democracy_score_model_norm <- lm(democracy_score ~ 
                                  gdp_growth_annual_percent + 
                                  gdp_per_capita_constant_2015_us  +
                                  tax_revenue_percent_of_gdp +
                                  total_natural_resources_rents_percent_of_gdp,
                                data = modelling_data_norm)

tidy(democracy_score_model_norm) 
glance(democracy_score_model_norm)
```

# Check assumptions
Check assumptions such as linearity, normality, and homoscedasticity to ensure that the model is appropriate for your data.


## Checking normality of the model using QQ-plot, Shapiro-Wilk test and Anderson-Darling test
```{r}
# Visualize histogram of residuals
hist(resid(democracy_score_model_norm))

# Visualize Q-Q plot of residuals
qqnorm(resid(democracy_score_model_norm))
qqline(resid(democracy_score_model_norm))

# Perform Shapiro-Wilk test
shapiro.test(resid(democracy_score_model_norm))

# Perform Anderson-Darling test
library(nortest)
ad.test(resid(democracy_score_model_norm))
```

## Checking homoscedasticity using a studentized Breusch-Pagan test and plotting the residuals vs predicted values
```{r}
# Plot residuals against predicted values
plot(predict(democracy_score_model_norm), resid(democracy_score_model_norm))

# Perform Breusch-Pagan test
library(lmtest)
bptest(democracy_score_model_norm)
```

## Checking linearity using the Cook-Weisberg test, in addition to the previously completed plots of the data
```{r}
# Perform Cook-Weisberg test
library(lmtest)
ncvTest(democracy_score_model_norm)
```

## Checking for outliers using Cook's distance
```{r}
cooks.distance(democracy_score_model_norm)

# Plot Cook's distance
plot(cooks.distance(democracy_score_model_norm), pch = 20, main = "Cook's distance")
abline(h = 4/length(worlddatabank_oecd_eea_numeric), col = "red")
```
